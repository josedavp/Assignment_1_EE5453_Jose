# AI Queries and Reflection

## Reflection 
*AI (ChatGPT) was used in setup troubleshooting, debugging Anaconda/Python configuration errors (PyTorch through Anaconda is not compatible with 5080 just yet. I learned how to properly configure and debug virtual environments, understand how CUDA errors are formed, verify how environments are interconnected, and verify deeper knowledge on my GPU and system hardware. It helped me verify the structure of the README through best academic/industry practices. While it did provide guidance I verified each type and confirmed the overall functionality and system design. It also helped in explaining the research paper from a more technical form to easier to digest version after I read the paper first. It provided explanations on the Transformer code, and helped explain line my line while creating a template TODO for me to work on. I verified each step, reviewed each line, and saw how a Transformer is correctly processed.*


## Key Queries: 
- How to set up and activate a Conda environment on a custom drive  
- How to fix CUDA / Anaconda compatibility issues for RTX 5080  
- Why `assert` is used in model.py  
- How weight initialization works and why we use `isinstance`  
- Explanation of the `forward()` pass (token + positional embeddings, blocks, logits, loss)  
- Explanation of the `generate()` function (temperature, top-k, sampling)  
- Training workflow (`train.py`) and checkpoints  
- How to structure README.md and results summary  
- Clarification on compute vs memory bound workloads (Part C)